{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Implement logistic regression and apply it to two different datasets\n",
    "**Task Overview:**\n",
    "- Logistic Regression \n",
    "- Regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Logistic Regression ##\n",
    "### 1.1 Packages\n",
    "\n",
    "Import useful packages for scientific computing and data processing. \n",
    "\n",
    "**Tasks:**\n",
    "1. Import numpy and rename it to np.\n",
    "2. Import pandas and rename it to pd.\n",
    "3. Import the pyplot function in the libraray of matplotlib and rename it to plt.\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and rename libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2 - Data Preparation ##\n",
    "\n",
    "Prepare the data for regression task.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load data for logistic regression.\n",
    "2. Generate the scatter plot of the data.\n",
    "\n",
    "1. The data file is \"data_logistic.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data and generate plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Sigmoid function ##\n",
    "\n",
    "\n",
    "Implement sigmoid function so it can be called by the rest of the program.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement the sigmoid function (**def sigmoid(z):**). \n",
    "2. Test the sigmoid function by function plotting with test data (X, Y) where Y = sigmoid(X). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement sigmoid fuction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Cost function and gradient ##\n",
    "\n",
    "Implement the cross entropy cost function and its gradient for logistic regression.\n",
    "    **Expected outputs:**\\\n",
    "    Cost at initial theta : 0.6931471805599445\\\n",
    "    Gradient at inital theta : [-0.1        -10.91242026 -11.73652937]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the cost function\n",
    "\n",
    "#def cal_cost(theta, X, y):\n",
    "    \n",
    "#    return J\n",
    "\n",
    "############################################\n",
    "#def cal_grad(theta, X, y):\n",
    "    \n",
    "#    return grad\n",
    "    \n",
    "    \n",
    "#print ('Cost at initial theta: {0}'.format(J))\n",
    "#print ('Gradient at inital theta:\\n {0}'.format(grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Train parameters with Gradient Descent ##\n",
    "\n",
    "\n",
    "Train parameters using Gradient Descent.\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate best fit theta by Gradient Descent with learning rate of **0.001** and epoch of **800K**. The initial theta from above blocks is used as initial values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Implementation\n",
    "\n",
    "\n",
    "\n",
    "#print ('Theta: {0}'.format(theta))\n",
    "#print ('Cost: {0}'.format(cal_cost(theta, X, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Decision Boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.6 Evaluating Logistic Regression\n",
    "\n",
    "Evaluate the model with given data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate the training accuracy and print it out\n",
    "3. Training accuracy should be high enough, like above 87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2 - Regularized Logistic Regression ##\n",
    "### 2.1 - Data Preparation\n",
    "\n",
    "Prepare the data for regression task.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load data for logistic regression.\n",
    "2. Generate the scatter plot of the data.\n",
    "\n",
    "1. The data file is \"data_reg_logistic.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Feature Mapping\n",
    "\n",
    "To obtain nonlinear boundary, map the features into higher dimension.\n",
    "\n",
    "**Tasks:**\n",
    "1. Map the 2-dimension features into all polynomial terms of x1 and x2 up to the 6th power.\n",
    "\n",
    "Validation for mapFeature(np.array([[0,1]])) is \\\n",
    "[[1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping fuction here: def mapFeatures(data, degree):\n",
    "\n",
    "#def mapFeature(data, degrees=6):\n",
    "#\n",
    "#    for ii in range(1, degrees+1):\n",
    "#        for jj in range(0,ii+1):\n",
    "#            X = np.hstack((X, (X1**(ii-jj) * X2**jj).reshape(m,1)))\n",
    "#    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Cost function and gradient\n",
    "\n",
    "Implement the cost function and gradient for regularized logistic regression.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement the \"cal_reg_cost\" to compute the cost.\n",
    "2. Implement the \"cal_reg_grad\" to compute the gradients.\n",
    "3. Test the the two functions with initial values.\n",
    "\n",
    "  **Validation**: \\\n",
    "  Cost at initial theta : 0.6931471805599442 \\\n",
    "  Gradient at inital theta : \\\n",
    "  [0.05    &nbsp;   0.03146256 &nbsp; 0.03589577 &nbsp;0.06512186 &nbsp;0.01044212\\\n",
    "  0.05812127 &nbsp; 0.02379224 &nbsp; 0.01121763 &nbsp; 0.01050091 &nbsp; 0.04185155\\\n",
    "  0.04795375 &nbsp; 0.00272593 &nbsp; 0.01551233 &nbsp; 0.00339255 &nbsp; 0.05331224\\\n",
    "  0.02385013 &nbsp; 0.00614284 &nbsp; 0.00382781 &nbsp; 0.00746595 &nbsp; 0.00566102\\\n",
    "  0.043769   &nbsp; 0.03690049 &nbsp; 0.00163929 &nbsp; 0.00723665 &nbsp; 0.00091111\\\n",
    "  0.00827412 &nbsp; 0.0017557  &nbsp; 0.05035145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the cost function\n",
    "\n",
    "# def cal_reg_cost(theta, X, y, lam):\n",
    "    \n",
    "#     # compute the hypothesis\n",
    "#     htheta = \n",
    "        \n",
    "#     # compute J in three terms\n",
    "#     term1 = \n",
    "#     term2 = \n",
    "#     term3 =  \n",
    "\n",
    "#     # cost function\n",
    "#     J = - 1 / m * (term1 + term2 - term3) \n",
    "    \n",
    "#     return J\n",
    "\n",
    "############################################\n",
    "# def cal_reg_grad(theta, X, y, lam):\n",
    "    \n",
    "#     # compute the hypothesis\n",
    "#     htheta = \n",
    "    \n",
    "#     # gradient\n",
    "#     grad = \n",
    "            \n",
    "#     return grad\n",
    "    \n",
    "\n",
    "\n",
    "# print ('Cost at initial theta: {0}'.format(J))\n",
    "# print ('Gradient at inital theta:\\n {0}'.format(grad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Train parameters with Gradient Descent ##\n",
    "\n",
    "\n",
    "Train parameters with Gradient Descent.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement GD function to optimize parameters.\n",
    "2. Print out the best theta and its corresponding cost.\n",
    "3. Plot the decision boundary.\n",
    "\n",
    "Validation results for first 3 epochs:\\\n",
    "------Epoch 0------\\\n",
    "Best fit theta: \\\n",
    "[-5.00000000e-05 -3.14625580e-05 -3.58957700e-05 -6.51218577e-05\n",
    " -1.04421167e-05 -5.81212666e-05 -2.37922390e-05 -1.12176321e-05\n",
    " -1.05009072e-05 -4.18515520e-05 -4.79537494e-05 -2.72592921e-06\n",
    " -1.55123328e-05 -3.39254609e-06 -5.33122440e-05 -2.38501348e-05\n",
    " -6.14284478e-06 -3.82780648e-06 -7.46594607e-06 -5.66102417e-06\n",
    " -4.37690001e-05 -3.69004876e-05 -1.63928681e-06 -7.23665148e-06\n",
    " -9.11107182e-07 -8.27412363e-06 -1.75570322e-06 -5.03514495e-05]\\\n",
    "Cost function at best fit theta: 0.6931200736183647\\\n",
    "------Epoch 1------\\\n",
    "Best fit theta:\\\n",
    "[-9.99660872e-05 -6.29180045e-05 -7.17762192e-05 -1.30232876e-04\n",
    " -2.08833542e-05 -1.16226611e-04 -4.75791514e-05 -2.24326894e-05\n",
    " -2.10002834e-05 -8.36911234e-05 -9.59008610e-05 -5.45120938e-06\n",
    " -3.10221132e-05 -6.78469307e-06 -1.06612424e-04 -4.76958444e-05\n",
    " -1.22845790e-05 -7.65483587e-06 -1.49304702e-05 -1.13213871e-05\n",
    " -8.75273807e-05 -7.37959678e-05 -3.27810252e-06 -1.44722935e-05\n",
    " -1.82196550e-06 -1.65468641e-05 -3.51116570e-06 -1.00692168e-04]\\\n",
    "Cost function at best fit theta: 0.6930929809256421\\\n",
    "------Epoch 2------\\\n",
    "Best fit theta: \\\n",
    "[-1.49898275e-04 -9.43663412e-05 -1.07641353e-04 -1.95333057e-04\n",
    " -3.13237127e-05 -1.74316038e-04 -7.13607385e-05 -3.36451729e-05\n",
    " -3.14981290e-05 -1.25518718e-04 -1.43841337e-04 -8.17584068e-06\n",
    " -4.65293421e-05 -1.01764410e-05 -1.59900545e-04 -7.15371301e-05\n",
    " -1.84252031e-05 -1.14810884e-05 -2.23935728e-05 -1.69810890e-05\n",
    " -1.31275145e-04 -1.10686442e-04 -4.91644726e-06 -2.17069264e-05\n",
    " -2.73257501e-06 -2.48182220e-05 -5.26638746e-06 -1.51022159e-04]\\\n",
    "Cost function at best fit theta: 0.6930659024719215\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning and plot\n",
    "\n",
    "    \n",
    "# print ('Best fit theta: {0}'.format(theta))\n",
    "# print ('Cost function at best fit theta: {0}'.format(cal_reg_cost(theta, X, y, lam)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Boundary Contour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Varying the Regularization parameter ##\n",
    "\n",
    "\n",
    "Modifying the value of regularization term lambda to see how the decision boundry changes.\n",
    "\n",
    "**Tasks:**\n",
    "1. Set lambda to be different values (0.01, 1, 10, 20) and plot the results.\n",
    "2. Print out the best lambda and the corresponding training accuracy.\n",
    "\n",
    "**Hints:**\n",
    "1. With the developed functions above, specify varying lambda values\n",
    "2. Initial thetas are the same.\n",
    "3. When lams = [0.01, 1, 10 , 20], you may obtain the similar plots below.\n",
    "![mxplusc](decision_boundary_set.png)\n",
    "4. It may take 3~4 mins to finish running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Varying the Regularization parameter\n",
    "\n",
    "# lams = [0.01, 1, 10, 20]\n",
    "# best_acc = 0\n",
    "# best_lam = -1\n",
    "# for ct, lam in enumerate(lams):\n",
    "\n",
    "\n",
    "# print('best acc',best_acc)\n",
    "# print('best lam',best_lam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
